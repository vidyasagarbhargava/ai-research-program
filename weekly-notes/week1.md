# Week 1 - Building Dataset

<table align="rights">
  <td align="center"><a target="_blank" href="[https://colab.research.google.com/drive/1h1Tp4swRd3Vmnfx5I4WeakSrIReZsUax#scrollTo=-Or0LmaZMh91](https://colab.research.google.com/drive/1GH8PW9-zAe4cXEZyOIE-T9uHXblIldAg?usp=sharing#scrollTo=-1RhGdeA8nVk)">
        <img src="https://i.ibb.co/2P3SLwK/colab.png"  style="padding-bottom:0.2px;" /></a></td>
</table>


## Dataset preparation
- Tokenization
- Data Sampling
- Data loading
- Embedding



## Resources

[1.How to Fine Tune an LLM Part 1 Preparing dataset for Instruction Tuning](https://wandb.ai/capecape/alpaca_ft/reports/How-to-Fine-Tune-an-LLM-Part-1-Preparing-a-Dataset-for-Instruction-Tuning--Vmlldzo1NTcxNzE2)  
[2.Generating a clinical instruction dataset in portuguese with langchain and gpt-4](https://solano-todeschini.medium.com/generating-a-clinical-instruction-dataset-in-portuguese-with-langchain-and-gpt-4-6ee9abfa41ae)  
[3. How I created an instruction dataset using gpt-3.5 to fine tune llama2 for new classification](https://medium.com/@kshitiz.sahay26/how-i-created-an-instruction-dataset-using-gpt-3-5-to-fine-tune-llama-2-for-news-classification-ed02fe41c81f)  
